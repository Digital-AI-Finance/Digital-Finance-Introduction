{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/Digital-Finance-Introduction/blob/main/day_02/notebooks/NB04_Credit_Scoring.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB04: Credit Scoring with Alternative Data\n",
    "\n",
    "**Topic 2.3: Data-Driven Finance**\n",
    "\n",
    "## Learning Objectives\n",
    "- Build a simple credit scoring model using machine learning\n",
    "- Understand how feature selection impacts predictions\n",
    "- Recognize potential sources of algorithmic bias\n",
    "- Explore the role of alternative data in modern finance\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Traditional credit scoring relies on:\n",
    "- Credit history\n",
    "- Income verification\n",
    "- Employment records\n",
    "\n",
    "**Alternative data** includes:\n",
    "- Mobile phone usage patterns\n",
    "- Social media activity\n",
    "- Utility bill payments\n",
    "- E-commerce behavior\n",
    "\n",
    "This notebook demonstrates how algorithms make lending decisions and where bias can creep in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Borrower Data\n",
    "\n",
    "We'll create a realistic dataset of loan applicants with:\n",
    "- **Demographic features**: Age, gender, location\n",
    "- **Traditional credit features**: Income, employment, credit history\n",
    "- **Alternative data features**: Mobile usage, social media, e-commerce behavior\n",
    "\n",
    "### Why Synthetic Data?\n",
    "\n",
    "Real credit data is highly sensitive and regulated. Synthetic data allows us to:\n",
    "1. Learn without privacy concerns\n",
    "2. Control and understand data relationships\n",
    "3. Explore bias scenarios safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_borrower_data(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Generate synthetic borrower data for credit scoring.\n",
    "    \n",
    "    The data includes realistic correlations between features and default probability.\n",
    "    Some features intentionally introduce potential for bias to demonstrate fairness issues.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of loan applicants to generate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with borrower features and default outcome\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # ======================\n",
    "    # Demographic Features\n",
    "    # ======================\n",
    "    \n",
    "    # Age: 18-70, normally distributed around 35\n",
    "    age = np.clip(np.random.normal(35, 12, n_samples), 18, 70).astype(int)\n",
    "    \n",
    "    # Gender: Binary for simplicity (in reality, more inclusive)\n",
    "    gender = np.random.choice(['Male', 'Female'], n_samples, p=[0.52, 0.48])\n",
    "    \n",
    "    # Location: Urban vs Rural (affects access to services)\n",
    "    location = np.random.choice(['Urban', 'Rural'], n_samples, p=[0.65, 0.35])\n",
    "    \n",
    "    # Region: Different economic conditions\n",
    "    region = np.random.choice(['North', 'South', 'East', 'West'], n_samples, \n",
    "                              p=[0.25, 0.30, 0.20, 0.25])\n",
    "    \n",
    "    # ======================\n",
    "    # Traditional Credit Features\n",
    "    # ======================\n",
    "    \n",
    "    # Annual income (correlated with age and location)\n",
    "    base_income = 30000 + (age - 18) * 800  # Income increases with age\n",
    "    location_multiplier = np.where(location == 'Urban', 1.2, 0.9)\n",
    "    income = (base_income * location_multiplier * \n",
    "              np.random.uniform(0.7, 1.3, n_samples)).astype(int)\n",
    "    \n",
    "    # Employment length (years, correlated with age)\n",
    "    max_employment = np.clip(age - 18, 0, 40)\n",
    "    employment_length = (max_employment * np.random.beta(2, 2, n_samples)).astype(int)\n",
    "    \n",
    "    # Existing debt-to-income ratio\n",
    "    debt_to_income = np.clip(np.random.exponential(0.25, n_samples), 0, 0.8)\n",
    "    \n",
    "    # Number of existing credit accounts\n",
    "    num_credit_accounts = np.random.poisson(3, n_samples)\n",
    "    \n",
    "    # Credit history length (months)\n",
    "    credit_history_months = np.clip(\n",
    "        np.random.normal(employment_length * 12 * 0.8, 24, n_samples), 0, 360\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Number of late payments in past 2 years\n",
    "    late_payments = np.random.poisson(0.8, n_samples)\n",
    "    \n",
    "    # Previous loan defaults (binary)\n",
    "    previous_default = np.random.choice([0, 1], n_samples, p=[0.92, 0.08])\n",
    "    \n",
    "    # Loan amount requested\n",
    "    loan_amount = np.clip(\n",
    "        income * np.random.uniform(0.3, 1.5, n_samples), 5000, 100000\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ======================\n",
    "    # Alternative Data Features\n",
    "    # ======================\n",
    "    \n",
    "    # Mobile phone usage (monthly bill amount) - proxy for stability\n",
    "    mobile_bill = np.clip(np.random.normal(50, 25, n_samples), 10, 200).astype(int)\n",
    "    \n",
    "    # Phone contract type (postpaid suggests stability)\n",
    "    phone_contract = np.random.choice(['Prepaid', 'Postpaid'], n_samples, \n",
    "                                       p=[0.4, 0.6])\n",
    "    \n",
    "    # Social media connections (network size)\n",
    "    social_connections = np.clip(\n",
    "        np.random.exponential(200, n_samples), 10, 2000\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Online shopping frequency (transactions per month)\n",
    "    online_shopping_freq = np.clip(\n",
    "        np.random.poisson(5, n_samples), 0, 30\n",
    "    )\n",
    "    \n",
    "    # Utility bill payment consistency (0-100%)\n",
    "    utility_payment_rate = np.clip(\n",
    "        np.random.beta(8, 2, n_samples), 0.5, 1.0\n",
    "    )\n",
    "    \n",
    "    # Device type (proxy for economic status - problematic!)\n",
    "    device_type = np.random.choice(['Budget', 'Mid-range', 'Premium'], n_samples,\n",
    "                                    p=[0.35, 0.45, 0.20])\n",
    "    \n",
    "    # App usage patterns (financial apps installed)\n",
    "    financial_apps = np.random.poisson(2, n_samples)\n",
    "    \n",
    "    # ======================\n",
    "    # Generate Default Outcome\n",
    "    # ======================\n",
    "    \n",
    "    # Calculate default probability based on features\n",
    "    # (This is a simplified model - real credit scoring is more complex)\n",
    "    \n",
    "    default_score = np.zeros(n_samples)\n",
    "    \n",
    "    # Traditional factors (strong predictors)\n",
    "    default_score += debt_to_income * 2.0  # Higher debt = higher risk\n",
    "    default_score += late_payments * 0.3  # Late payments increase risk\n",
    "    default_score += previous_default * 1.5  # Previous default is major risk\n",
    "    default_score -= employment_length * 0.03  # Employment stability reduces risk\n",
    "    default_score -= np.log1p(income) * 0.15  # Higher income reduces risk\n",
    "    default_score -= credit_history_months * 0.002  # Longer history reduces risk\n",
    "    \n",
    "    # Alternative data factors (moderate predictors)\n",
    "    default_score -= utility_payment_rate * 0.5  # Consistent payments reduce risk\n",
    "    default_score -= np.where(phone_contract == 'Postpaid', 0.2, 0)  # Postpaid = stability\n",
    "    default_score -= financial_apps * 0.05  # Financial awareness reduces risk\n",
    "    \n",
    "    # Add noise\n",
    "    default_score += np.random.normal(0, 0.3, n_samples)\n",
    "    \n",
    "    # Convert to probability and then binary outcome\n",
    "    default_prob = 1 / (1 + np.exp(-default_score + 0.5))  # Sigmoid\n",
    "    default = (np.random.random(n_samples) < default_prob).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        # Demographics\n",
    "        'age': age,\n",
    "        'gender': gender,\n",
    "        'location': location,\n",
    "        'region': region,\n",
    "        \n",
    "        # Traditional credit features\n",
    "        'annual_income': income,\n",
    "        'employment_length': employment_length,\n",
    "        'debt_to_income': debt_to_income.round(3),\n",
    "        'num_credit_accounts': num_credit_accounts,\n",
    "        'credit_history_months': credit_history_months,\n",
    "        'late_payments': late_payments,\n",
    "        'previous_default': previous_default,\n",
    "        'loan_amount': loan_amount,\n",
    "        \n",
    "        # Alternative data\n",
    "        'mobile_bill': mobile_bill,\n",
    "        'phone_contract': phone_contract,\n",
    "        'social_connections': social_connections,\n",
    "        'online_shopping_freq': online_shopping_freq,\n",
    "        'utility_payment_rate': utility_payment_rate.round(3),\n",
    "        'device_type': device_type,\n",
    "        'financial_apps': financial_apps,\n",
    "        \n",
    "        # Outcome\n",
    "        'default': default\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_borrower_data(n_samples=2000)\n",
    "\n",
    "print(\"Borrower Dataset Generated\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total applicants: {len(df):,}\")\n",
    "print(f\"Default rate: {df['default'].mean():.1%}\")\n",
    "print(f\"\\nFeatures: {len(df.columns) - 1}\")\n",
    "print(f\"  - Demographics: 4\")\n",
    "print(f\"  - Traditional credit: 8\")\n",
    "print(f\"  - Alternative data: 7\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "print(\"\\nCategorical Features:\")\n",
    "for col in ['gender', 'location', 'region', 'phone_contract', 'device_type']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Before building models, we need to understand our data:\n",
    "- Feature distributions\n",
    "- Correlations between features\n",
    "- Relationships with the default outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Income distribution by default status\n",
    "ax = axes[0, 0]\n",
    "for default_val, label in [(0, 'No Default'), (1, 'Default')]:\n",
    "    subset = df[df['default'] == default_val]['annual_income'] / 1000\n",
    "    ax.hist(subset, bins=30, alpha=0.6, label=label, density=True)\n",
    "ax.set_xlabel('Annual Income ($K)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Income Distribution by Default Status')\n",
    "ax.legend()\n",
    "\n",
    "# Age distribution\n",
    "ax = axes[0, 1]\n",
    "for default_val, label in [(0, 'No Default'), (1, 'Default')]:\n",
    "    subset = df[df['default'] == default_val]['age']\n",
    "    ax.hist(subset, bins=25, alpha=0.6, label=label, density=True)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Age Distribution by Default Status')\n",
    "ax.legend()\n",
    "\n",
    "# Debt-to-income ratio\n",
    "ax = axes[0, 2]\n",
    "for default_val, label in [(0, 'No Default'), (1, 'Default')]:\n",
    "    subset = df[df['default'] == default_val]['debt_to_income']\n",
    "    ax.hist(subset, bins=30, alpha=0.6, label=label, density=True)\n",
    "ax.set_xlabel('Debt-to-Income Ratio')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('DTI Distribution by Default Status')\n",
    "ax.legend()\n",
    "\n",
    "# Late payments\n",
    "ax = axes[1, 0]\n",
    "default_rates = df.groupby('late_payments')['default'].mean()\n",
    "ax.bar(default_rates.index, default_rates.values, color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Number of Late Payments')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Late Payments')\n",
    "\n",
    "# Utility payment rate\n",
    "ax = axes[1, 1]\n",
    "df['utility_bin'] = pd.cut(df['utility_payment_rate'], bins=5)\n",
    "utility_rates = df.groupby('utility_bin', observed=True)['default'].mean()\n",
    "ax.bar(range(len(utility_rates)), utility_rates.values, color='steelblue', edgecolor='black')\n",
    "ax.set_xticks(range(len(utility_rates)))\n",
    "ax.set_xticklabels([f'{x.left:.0%}-{x.right:.0%}' for x in utility_rates.index], rotation=45)\n",
    "ax.set_xlabel('Utility Payment Rate')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Utility Payment Consistency')\n",
    "\n",
    "# Device type (alternative data)\n",
    "ax = axes[1, 2]\n",
    "device_rates = df.groupby('device_type')['default'].mean().reindex(['Budget', 'Mid-range', 'Premium'])\n",
    "ax.bar(device_rates.index, device_rates.values, color=['#ff9999', '#99ccff', '#99ff99'], edgecolor='black')\n",
    "ax.set_xlabel('Device Type')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Device Type (Alternative Data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('utility_bin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# Select numerical features for correlation\n",
    "numerical_cols = ['age', 'annual_income', 'employment_length', 'debt_to_income',\n",
    "                  'num_credit_accounts', 'credit_history_months', 'late_payments',\n",
    "                  'previous_default', 'loan_amount', 'mobile_bill', 'social_connections',\n",
    "                  'online_shopping_freq', 'utility_payment_rate', 'financial_apps', 'default']\n",
    "\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            linewidths=0.5, square=True)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with default\n",
    "print(\"\\nCorrelations with Default (sorted by absolute value):\")\n",
    "print(\"=\" * 50)\n",
    "default_corr = correlation_matrix['default'].drop('default').sort_values(key=abs, ascending=False)\n",
    "for feature, corr in default_corr.items():\n",
    "    direction = 'increases' if corr > 0 else 'decreases'\n",
    "    print(f\"  {feature:<25} {corr:>6.3f}  (higher value {direction} default risk)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic breakdown of default rates\n",
    "print(\"Default Rates by Demographic Groups\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# By gender\n",
    "ax = axes[0]\n",
    "gender_rates = df.groupby('gender')['default'].agg(['mean', 'count'])\n",
    "bars = ax.bar(gender_rates.index, gender_rates['mean'], color=['#ff9999', '#99ccff'], edgecolor='black')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Gender')\n",
    "for bar, count in zip(bars, gender_rates['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'n={count}', ha='center', fontsize=9)\n",
    "\n",
    "# By location\n",
    "ax = axes[1]\n",
    "location_rates = df.groupby('location')['default'].agg(['mean', 'count'])\n",
    "bars = ax.bar(location_rates.index, location_rates['mean'], color=['#99ff99', '#ffcc99'], edgecolor='black')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Location')\n",
    "for bar, count in zip(bars, location_rates['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'n={count}', ha='center', fontsize=9)\n",
    "\n",
    "# By region\n",
    "ax = axes[2]\n",
    "region_rates = df.groupby('region')['default'].agg(['mean', 'count'])\n",
    "bars = ax.bar(region_rates.index, region_rates['mean'], color=sns.color_palette('Set2', 4), edgecolor='black')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Region')\n",
    "for bar, count in zip(bars, region_rates['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'n={count}', ha='center', fontsize=9)\n",
    "\n",
    "# By age group\n",
    "ax = axes[3]\n",
    "df['age_group'] = pd.cut(df['age'], bins=[18, 25, 35, 45, 55, 70], \n",
    "                         labels=['18-25', '26-35', '36-45', '46-55', '56-70'])\n",
    "age_rates = df.groupby('age_group', observed=True)['default'].agg(['mean', 'count'])\n",
    "bars = ax.bar(age_rates.index, age_rates['mean'], color=sns.color_palette('viridis', 5), edgecolor='black')\n",
    "ax.set_ylabel('Default Rate')\n",
    "ax.set_title('Default Rate by Age Group')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "for bar, count in zip(bars, age_rates['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'n={count}', ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the statistics\n",
    "print(\"\\nGender:\")\n",
    "print(gender_rates.round(3))\n",
    "print(\"\\nLocation:\")\n",
    "print(location_rates.round(3))\n",
    "print(\"\\nRegion:\")\n",
    "print(region_rates.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional Credit Score Model\n",
    "\n",
    "First, we'll build a credit scoring model using only **traditional features** that are commonly used in credit decisions:\n",
    "- Income\n",
    "- Employment history\n",
    "- Credit history\n",
    "- Existing debt\n",
    "\n",
    "We'll use **Logistic Regression** - a standard model in credit scoring because:\n",
    "1. It's interpretable (we can see feature importance)\n",
    "2. It outputs probabilities\n",
    "3. It's required by regulations in many jurisdictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define traditional features (no alternative data)\n",
    "traditional_features = [\n",
    "    'annual_income',\n",
    "    'employment_length',\n",
    "    'debt_to_income',\n",
    "    'num_credit_accounts',\n",
    "    'credit_history_months',\n",
    "    'late_payments',\n",
    "    'previous_default',\n",
    "    'loan_amount'\n",
    "]\n",
    "\n",
    "print(\"Traditional Credit Features:\")\n",
    "print(\"=\" * 50)\n",
    "for i, feature in enumerate(traditional_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "# Prepare data\n",
    "X_traditional = df[traditional_features].copy()\n",
    "y = df['default'].copy()\n",
    "\n",
    "# Train-test split (stratified to maintain class balance)\n",
    "X_train_trad, X_test_trad, y_train, y_test = train_test_split(\n",
    "    X_traditional, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train_trad)} samples\")\n",
    "print(f\"Test set: {len(X_test_trad)} samples\")\n",
    "print(f\"Default rate in training: {y_train.mean():.1%}\")\n",
    "print(f\"Default rate in test: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for logistic regression\n",
    "scaler_trad = StandardScaler()\n",
    "X_train_trad_scaled = scaler_trad.fit_transform(X_train_trad)\n",
    "X_test_trad_scaled = scaler_trad.transform(X_test_trad)\n",
    "\n",
    "# Train logistic regression model\n",
    "model_traditional = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_traditional.fit(X_train_trad_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_trad = model_traditional.predict(X_test_trad_scaled)\n",
    "y_prob_trad = model_traditional.predict_proba(X_test_trad_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Traditional Model Performance\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_trad, target_names=['No Default', 'Default']))\n",
    "\n",
    "# Calculate AUC\n",
    "auc_traditional = roc_auc_score(y_test, y_prob_trad)\n",
    "print(f\"\\nAUC-ROC Score: {auc_traditional:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for traditional model\n",
    "feature_importance_trad = pd.DataFrame({\n",
    "    'Feature': traditional_features,\n",
    "    'Coefficient': model_traditional.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(model_traditional.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if x < 0 else 'red' for x in feature_importance_trad['Coefficient']]\n",
    "plt.barh(feature_importance_trad['Feature'], feature_importance_trad['Coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient (Positive = Increases Default Risk)')\n",
    "plt.title('Traditional Model: Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Interpretation:\")\n",
    "print(\"=\" * 50)\n",
    "for _, row in feature_importance_trad.sort_values('Abs_Coefficient', ascending=False).iterrows():\n",
    "    direction = 'INCREASES' if row['Coefficient'] > 0 else 'DECREASES'\n",
    "    print(f\"  {row['Feature']:<25} {direction} default risk (coef: {row['Coefficient']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternative Data Model\n",
    "\n",
    "Now let's add **alternative data features** to our model. These are increasingly used by:\n",
    "- FinTech lenders\n",
    "- Microfinance institutions\n",
    "- Developing market lenders\n",
    "\n",
    "### Benefits of Alternative Data:\n",
    "- Can score \"thin file\" applicants (limited credit history)\n",
    "- More real-time signals\n",
    "- Potentially better predictions\n",
    "\n",
    "### Risks of Alternative Data:\n",
    "- Privacy concerns\n",
    "- Potential for bias (e.g., device type correlates with income)\n",
    "- Less regulatory clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alternative data features\n",
    "alternative_features = [\n",
    "    'mobile_bill',\n",
    "    'social_connections',\n",
    "    'online_shopping_freq',\n",
    "    'utility_payment_rate',\n",
    "    'financial_apps'\n",
    "]\n",
    "\n",
    "# Encode categorical alternative features\n",
    "df_encoded = df.copy()\n",
    "df_encoded['phone_contract_postpaid'] = (df_encoded['phone_contract'] == 'Postpaid').astype(int)\n",
    "df_encoded['device_premium'] = (df_encoded['device_type'] == 'Premium').astype(int)\n",
    "df_encoded['device_budget'] = (df_encoded['device_type'] == 'Budget').astype(int)\n",
    "\n",
    "# Combined features: traditional + alternative\n",
    "all_features = traditional_features + alternative_features + [\n",
    "    'phone_contract_postpaid', 'device_premium', 'device_budget'\n",
    "]\n",
    "\n",
    "print(\"Combined Feature Set (Traditional + Alternative):\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTraditional Features:\")\n",
    "for f in traditional_features:\n",
    "    print(f\"  - {f}\")\n",
    "print(\"\\nAlternative Data Features:\")\n",
    "for f in alternative_features + ['phone_contract_postpaid', 'device_premium', 'device_budget']:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare combined dataset\n",
    "X_combined = df_encoded[all_features].copy()\n",
    "\n",
    "# Use same train-test indices for fair comparison\n",
    "X_train_comb, X_test_comb, _, _ = train_test_split(\n",
    "    X_combined, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler_comb = StandardScaler()\n",
    "X_train_comb_scaled = scaler_comb.fit_transform(X_train_comb)\n",
    "X_test_comb_scaled = scaler_comb.transform(X_test_comb)\n",
    "\n",
    "# Train combined model\n",
    "model_combined = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_combined.fit(X_train_comb_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_comb = model_combined.predict(X_test_comb_scaled)\n",
    "y_prob_comb = model_combined.predict_proba(X_test_comb_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Combined Model Performance (Traditional + Alternative Data)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_comb, target_names=['No Default', 'Default']))\n",
    "\n",
    "# Calculate AUC\n",
    "auc_combined = roc_auc_score(y_test, y_prob_comb)\n",
    "print(f\"\\nAUC-ROC Score: {auc_combined:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for combined model\n",
    "feature_importance_comb = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Coefficient': model_combined.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(model_combined.coef_[0]),\n",
    "    'Type': ['Traditional'] * len(traditional_features) + \n",
    "            ['Alternative'] * (len(all_features) - len(traditional_features))\n",
    "}).sort_values('Abs_Coefficient', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#2ecc71' if row['Type'] == 'Traditional' and row['Coefficient'] < 0 else\n",
    "          '#e74c3c' if row['Type'] == 'Traditional' and row['Coefficient'] > 0 else\n",
    "          '#3498db' if row['Type'] == 'Alternative' and row['Coefficient'] < 0 else\n",
    "          '#9b59b6' for _, row in feature_importance_comb.iterrows()]\n",
    "\n",
    "bars = plt.barh(feature_importance_comb['Feature'], feature_importance_comb['Coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient (Positive = Increases Default Risk)')\n",
    "plt.title('Combined Model: Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='Traditional (reduces risk)'),\n",
    "    Patch(facecolor='#e74c3c', label='Traditional (increases risk)'),\n",
    "    Patch(facecolor='#3498db', label='Alternative (reduces risk)'),\n",
    "    Patch(facecolor='#9b59b6', label='Alternative (increases risk)')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Let's compare the traditional and alternative data models using:\n",
    "- **ROC Curves**: Trade-off between true positive and false positive rates\n",
    "- **AUC Scores**: Area Under the ROC Curve (higher is better)\n",
    "- **Precision-Recall**: Important for imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curves\n",
    "fpr_trad, tpr_trad, thresholds_trad = roc_curve(y_test, y_prob_trad)\n",
    "fpr_comb, tpr_comb, thresholds_comb = roc_curve(y_test, y_prob_comb)\n",
    "\n",
    "# Plot ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve comparison\n",
    "ax = axes[0]\n",
    "ax.plot(fpr_trad, tpr_trad, label=f'Traditional Model (AUC = {auc_traditional:.3f})', \n",
    "        linewidth=2, color='steelblue')\n",
    "ax.plot(fpr_comb, tpr_comb, label=f'Combined Model (AUC = {auc_combined:.3f})', \n",
    "        linewidth=2, color='coral')\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# AUC improvement visualization\n",
    "ax = axes[1]\n",
    "models = ['Traditional\\nModel', 'Combined Model\\n(+Alternative Data)']\n",
    "aucs = [auc_traditional, auc_combined]\n",
    "colors = ['steelblue', 'coral']\n",
    "bars = ax.bar(models, aucs, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('AUC-ROC Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{auc:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add improvement annotation\n",
    "improvement = (auc_combined - auc_traditional) / auc_traditional * 100\n",
    "ax.annotate(f'+{improvement:.1f}%', xy=(1, auc_combined), xytext=(1.3, (auc_combined + auc_traditional)/2),\n",
    "            fontsize=12, color='green', fontweight='bold',\n",
    "            arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Comparison Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Traditional Model AUC: {auc_traditional:.4f}\")\n",
    "print(f\"Combined Model AUC:    {auc_combined:.4f}\")\n",
    "print(f\"Improvement:           +{improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Traditional model confusion matrix\n",
    "cm_trad = confusion_matrix(y_test, y_pred_trad)\n",
    "sns.heatmap(cm_trad, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Default', 'Default'],\n",
    "            yticklabels=['No Default', 'Default'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Traditional Model\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Combined model confusion matrix\n",
    "cm_comb = confusion_matrix(y_test, y_pred_comb)\n",
    "sns.heatmap(cm_comb, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['No Default', 'Default'],\n",
    "            yticklabels=['No Default', 'Default'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Combined Model\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics\n",
    "print(\"\\nError Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTraditional Model:\")\n",
    "print(f\"  False Positives (denied good borrowers): {cm_trad[0, 1]}\")\n",
    "print(f\"  False Negatives (approved bad borrowers): {cm_trad[1, 0]}\")\n",
    "print(f\"\\nCombined Model:\")\n",
    "print(f\"  False Positives (denied good borrowers): {cm_comb[0, 1]}\")\n",
    "print(f\"  False Negatives (approved bad borrowers): {cm_comb[1, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Algorithmic Bias Analysis\n",
    "\n",
    "Credit scoring algorithms can inadvertently discriminate against protected groups. This can happen through:\n",
    "\n",
    "1. **Direct discrimination**: Using protected attributes (illegal)\n",
    "2. **Proxy discrimination**: Using features correlated with protected attributes\n",
    "3. **Historical bias**: Training on biased historical data\n",
    "\n",
    "Let's analyze our models for potential bias across demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test data for bias analysis\n",
    "test_df = df.iloc[X_test_trad.index].copy()\n",
    "test_df['pred_traditional'] = y_pred_trad\n",
    "test_df['prob_traditional'] = y_prob_trad\n",
    "test_df['pred_combined'] = y_pred_comb\n",
    "test_df['prob_combined'] = y_prob_comb\n",
    "test_df['actual'] = y_test.values\n",
    "\n",
    "print(\"Bias Analysis Setup\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"\\nProtected groups analyzed:\")\n",
    "print(\"  - Gender (Male/Female)\")\n",
    "print(\"  - Location (Urban/Rural)\")\n",
    "print(\"  - Age Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_group_bias(df, group_col, actual_col, pred_col, prob_col):\n",
    "    \"\"\"\n",
    "    Analyze model bias across a demographic group.\n",
    "    \n",
    "    Returns metrics for each group:\n",
    "    - Approval rate (predicted as no default)\n",
    "    - Actual default rate\n",
    "    - False positive rate\n",
    "    - False negative rate\n",
    "    - Average predicted probability\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for group in df[group_col].unique():\n",
    "        group_df = df[df[group_col] == group]\n",
    "        \n",
    "        n = len(group_df)\n",
    "        approval_rate = (group_df[pred_col] == 0).mean()  # Approved = predicted no default\n",
    "        actual_default_rate = group_df[actual_col].mean()\n",
    "        avg_prob = group_df[prob_col].mean()\n",
    "        \n",
    "        # Calculate FPR and FNR\n",
    "        true_neg = ((group_df[actual_col] == 0) & (group_df[pred_col] == 0)).sum()\n",
    "        false_pos = ((group_df[actual_col] == 0) & (group_df[pred_col] == 1)).sum()\n",
    "        true_pos = ((group_df[actual_col] == 1) & (group_df[pred_col] == 1)).sum()\n",
    "        false_neg = ((group_df[actual_col] == 1) & (group_df[pred_col] == 0)).sum()\n",
    "        \n",
    "        fpr = false_pos / (false_pos + true_neg) if (false_pos + true_neg) > 0 else 0\n",
    "        fnr = false_neg / (false_neg + true_pos) if (false_neg + true_pos) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Group': group,\n",
    "            'N': n,\n",
    "            'Approval_Rate': approval_rate,\n",
    "            'Actual_Default_Rate': actual_default_rate,\n",
    "            'Avg_Risk_Score': avg_prob,\n",
    "            'FPR': fpr,\n",
    "            'FNR': fnr\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze bias by gender\n",
    "print(\"Bias Analysis: Gender\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gender_bias_trad = analyze_group_bias(test_df, 'gender', 'actual', 'pred_traditional', 'prob_traditional')\n",
    "gender_bias_comb = analyze_group_bias(test_df, 'gender', 'actual', 'pred_combined', 'prob_combined')\n",
    "\n",
    "print(\"\\nTraditional Model:\")\n",
    "print(gender_bias_trad.round(3).to_string(index=False))\n",
    "print(\"\\nCombined Model (with Alternative Data):\")\n",
    "print(gender_bias_comb.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias across demographic groups\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Gender bias\n",
    "gender_bias_trad_plot = gender_bias_trad.set_index('Group')\n",
    "gender_bias_comb_plot = gender_bias_comb.set_index('Group')\n",
    "\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(gender_bias_trad_plot))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, gender_bias_trad_plot['Approval_Rate'], width, label='Traditional', color='steelblue')\n",
    "ax.bar(x + width/2, gender_bias_comb_plot['Approval_Rate'], width, label='Combined', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(gender_bias_trad_plot.index)\n",
    "ax.set_ylabel('Approval Rate')\n",
    "ax.set_title('Approval Rate by Gender', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.axhline(y=test_df['pred_traditional'].mean(), color='steelblue', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Location bias\n",
    "location_bias_trad = analyze_group_bias(test_df, 'location', 'actual', 'pred_traditional', 'prob_traditional')\n",
    "location_bias_comb = analyze_group_bias(test_df, 'location', 'actual', 'pred_combined', 'prob_combined')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(location_bias_trad))\n",
    "ax.bar(x - width/2, location_bias_trad['Approval_Rate'], width, label='Traditional', color='steelblue')\n",
    "ax.bar(x + width/2, location_bias_comb['Approval_Rate'], width, label='Combined', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(location_bias_trad['Group'])\n",
    "ax.set_ylabel('Approval Rate')\n",
    "ax.set_title('Approval Rate by Location', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Region bias\n",
    "region_bias_trad = analyze_group_bias(test_df, 'region', 'actual', 'pred_traditional', 'prob_traditional')\n",
    "region_bias_comb = analyze_group_bias(test_df, 'region', 'actual', 'pred_combined', 'prob_combined')\n",
    "\n",
    "ax = axes[0, 2]\n",
    "x = np.arange(len(region_bias_trad))\n",
    "ax.bar(x - width/2, region_bias_trad['Approval_Rate'], width, label='Traditional', color='steelblue')\n",
    "ax.bar(x + width/2, region_bias_comb['Approval_Rate'], width, label='Combined', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(region_bias_trad['Group'])\n",
    "ax.set_ylabel('Approval Rate')\n",
    "ax.set_title('Approval Rate by Region', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# False Positive Rates (denied good borrowers)\n",
    "ax = axes[1, 0]\n",
    "ax.bar(x - width/2, gender_bias_trad_plot['FPR'], width, label='Traditional', color='steelblue')\n",
    "ax.bar(x + width/2, gender_bias_comb_plot['FPR'], width, label='Combined', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(gender_bias_trad_plot.index)\n",
    "ax.set_ylabel('False Positive Rate')\n",
    "ax.set_title('FPR by Gender\\n(Good borrowers denied)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# False Negative Rates (approved bad borrowers)\n",
    "ax = axes[1, 1]\n",
    "ax.bar(x - width/2, gender_bias_trad_plot['FNR'], width, label='Traditional', color='steelblue')\n",
    "ax.bar(x + width/2, gender_bias_comb_plot['FNR'], width, label='Combined', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(gender_bias_trad_plot.index)\n",
    "ax.set_ylabel('False Negative Rate')\n",
    "ax.set_title('FNR by Gender\\n(Bad borrowers approved)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Average risk scores\n",
    "ax = axes[1, 2]\n",
    "ax.bar(x - width/2, gender_bias_trad_plot['Avg_Risk_Score'], width, label='Traditional', color='steelblue')\n",
    "ax.bar(x + width/2, gender_bias_comb_plot['Avg_Risk_Score'], width, label='Combined', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(gender_bias_trad_plot.index)\n",
    "ax.set_ylabel('Average Risk Score')\n",
    "ax.set_title('Average Predicted Risk by Gender', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed bias report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALGORITHMIC BIAS REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def calculate_disparity(df_bias, metric='Approval_Rate'):\n",
    "    \"\"\"Calculate disparity ratio between groups.\"\"\"\n",
    "    values = df_bias[metric].values\n",
    "    if len(values) == 2:\n",
    "        return min(values) / max(values)\n",
    "    return values.min() / values.max()\n",
    "\n",
    "print(\"\\n1. GENDER DISPARITIES\")\n",
    "print(\"-\" * 50)\n",
    "approval_disparity_trad = calculate_disparity(gender_bias_trad)\n",
    "approval_disparity_comb = calculate_disparity(gender_bias_comb)\n",
    "print(f\"Traditional Model - Approval Rate Ratio: {approval_disparity_trad:.3f}\")\n",
    "print(f\"Combined Model - Approval Rate Ratio: {approval_disparity_comb:.3f}\")\n",
    "print(f\"(Ratio of 1.0 = perfect parity, <0.8 may indicate bias)\")\n",
    "\n",
    "print(\"\\n2. LOCATION DISPARITIES (Urban vs Rural)\")\n",
    "print(\"-\" * 50)\n",
    "loc_disparity_trad = calculate_disparity(location_bias_trad)\n",
    "loc_disparity_comb = calculate_disparity(location_bias_comb)\n",
    "print(f\"Traditional Model - Approval Rate Ratio: {loc_disparity_trad:.3f}\")\n",
    "print(f\"Combined Model - Approval Rate Ratio: {loc_disparity_comb:.3f}\")\n",
    "\n",
    "print(\"\\n3. REGIONAL DISPARITIES\")\n",
    "print(\"-\" * 50)\n",
    "reg_disparity_trad = calculate_disparity(region_bias_trad)\n",
    "reg_disparity_comb = calculate_disparity(region_bias_comb)\n",
    "print(f\"Traditional Model - Approval Rate Ratio: {reg_disparity_trad:.3f}\")\n",
    "print(f\"Combined Model - Approval Rate Ratio: {reg_disparity_comb:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. Adding alternative data can either increase or decrease disparities\n",
    "   depending on how the alternative features correlate with protected groups.\n",
    "\n",
    "2. Features like 'device_type' may serve as proxies for economic status,\n",
    "   which can correlate with protected characteristics.\n",
    "\n",
    "3. Location-based disparities may reflect historical inequalities in\n",
    "   access to financial services (financial exclusion).\n",
    "\n",
    "4. The \"four-fifths rule\" (80% rule) is a common legal threshold:\n",
    "   if a protected group's approval rate is less than 80% of the\n",
    "   highest group, adverse impact may be present.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fairness Metrics\n",
    "\n",
    "There are several mathematical definitions of fairness. Unfortunately, they often conflict with each other!\n",
    "\n",
    "### Key Fairness Metrics:\n",
    "\n",
    "1. **Statistical Parity**: Equal approval rates across groups\n",
    "2. **Equalized Odds**: Equal TPR and FPR across groups  \n",
    "3. **Predictive Parity**: Equal precision across groups\n",
    "4. **Calibration**: Predicted probabilities match actual rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness_metrics(df, group_col, actual_col, pred_col, prob_col):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive fairness metrics for a model.\n",
    "    \"\"\"\n",
    "    groups = df[group_col].unique()\n",
    "    metrics = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        group_df = df[df[group_col] == group]\n",
    "        \n",
    "        # Basic counts\n",
    "        n = len(group_df)\n",
    "        tp = ((group_df[actual_col] == 1) & (group_df[pred_col] == 1)).sum()\n",
    "        tn = ((group_df[actual_col] == 0) & (group_df[pred_col] == 0)).sum()\n",
    "        fp = ((group_df[actual_col] == 0) & (group_df[pred_col] == 1)).sum()\n",
    "        fn = ((group_df[actual_col] == 1) & (group_df[pred_col] == 0)).sum()\n",
    "        \n",
    "        # Rates\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity/Recall\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        approval_rate = (group_df[pred_col] == 0).mean()  # Approved = no default predicted\n",
    "        \n",
    "        metrics[group] = {\n",
    "            'n': n,\n",
    "            'approval_rate': approval_rate,  # Statistical Parity\n",
    "            'tpr': tpr,  # Equalized Odds - component 1\n",
    "            'fpr': fpr,  # Equalized Odds - component 2\n",
    "            'precision': precision,  # Predictive Parity\n",
    "            'base_rate': group_df[actual_col].mean()  # For calibration context\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(metrics).T\n",
    "\n",
    "# Calculate fairness metrics for both models\n",
    "print(\"Fairness Metrics Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fairness_trad_gender = calculate_fairness_metrics(\n",
    "    test_df, 'gender', 'actual', 'pred_traditional', 'prob_traditional'\n",
    ")\n",
    "fairness_comb_gender = calculate_fairness_metrics(\n",
    "    test_df, 'gender', 'actual', 'pred_combined', 'prob_combined'\n",
    ")\n",
    "\n",
    "print(\"\\nTraditional Model - Gender Fairness:\")\n",
    "print(fairness_trad_gender.round(3))\n",
    "\n",
    "print(\"\\nCombined Model - Gender Fairness:\")\n",
    "print(fairness_comb_gender.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fairness metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Prepare data for plotting\n",
    "metrics_to_plot = ['approval_rate', 'tpr', 'fpr', 'precision']\n",
    "metric_names = ['Statistical Parity\\n(Approval Rate)', 'True Positive Rate\\n(Catches defaults)', \n",
    "                'False Positive Rate\\n(Denies good borrowers)', 'Predictive Parity\\n(Precision)']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    groups = fairness_trad_gender.index\n",
    "    x = np.arange(len(groups))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, fairness_trad_gender[metric], width, \n",
    "           label='Traditional', color='steelblue', edgecolor='black')\n",
    "    ax.bar(x + width/2, fairness_comb_gender[metric], width, \n",
    "           label='Combined', color='coral', edgecolor='black')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add parity line\n",
    "    if metric == 'approval_rate':\n",
    "        overall_rate = test_df['pred_traditional'].mean()\n",
    "        ax.axhline(y=1-overall_rate, color='gray', linestyle='--', alpha=0.5, label='Overall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display fairness disparities\n",
    "def calculate_fairness_disparities(fairness_df):\n",
    "    \"\"\"\n",
    "    Calculate disparity ratios for each fairness metric.\n",
    "    Closer to 1.0 = more fair\n",
    "    \"\"\"\n",
    "    disparities = {}\n",
    "    for col in fairness_df.columns:\n",
    "        if col != 'n':\n",
    "            values = fairness_df[col].values\n",
    "            if max(values) > 0:\n",
    "                disparities[col] = min(values) / max(values)\n",
    "            else:\n",
    "                disparities[col] = 1.0\n",
    "    return disparities\n",
    "\n",
    "print(\"\\nFairness Disparity Ratios (closer to 1.0 = more fair)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThreshold: Values < 0.80 may indicate unfair treatment (four-fifths rule)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "disp_trad = calculate_fairness_disparities(fairness_trad_gender)\n",
    "disp_comb = calculate_fairness_disparities(fairness_comb_gender)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Traditional':<15} {'Combined':<15} {'Status'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric in disp_trad.keys():\n",
    "    trad_val = disp_trad[metric]\n",
    "    comb_val = disp_comb[metric]\n",
    "    \n",
    "    # Determine which is better\n",
    "    better = \"Combined\" if comb_val > trad_val else \"Traditional\" if trad_val > comb_val else \"Equal\"\n",
    "    \n",
    "    # Check if either violates four-fifths rule\n",
    "    warning = \"\"\n",
    "    if trad_val < 0.80 or comb_val < 0.80:\n",
    "        warning = \" [!]\"\n",
    "    \n",
    "    print(f\"{metric:<25} {trad_val:<15.3f} {comb_val:<15.3f} {better}{warning}\")\n",
    "\n",
    "print(\"\\n[!] = Below 0.80 threshold (potential adverse impact)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Fairness-Accuracy Trade-off\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"THE FAIRNESS-ACCURACY TRADE-OFF\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "A fundamental challenge in fair ML: we often cannot satisfy all fairness\n",
    "criteria simultaneously while maximizing accuracy.\n",
    "\n",
    "Key Trade-offs:\n",
    "\n",
    "1. STATISTICAL PARITY vs ACCURACY\n",
    "   - Enforcing equal approval rates may require approving higher-risk\n",
    "     applicants from disadvantaged groups, reducing accuracy.\n",
    "\n",
    "2. EQUALIZED ODDS vs STATISTICAL PARITY\n",
    "   - If base rates differ between groups (they often do due to\n",
    "     historical inequality), equal TPR/FPR means unequal approval rates.\n",
    "\n",
    "3. INDIVIDUAL vs GROUP FAIRNESS\n",
    "   - Group fairness (equal rates across groups) may conflict with\n",
    "     treating similar individuals similarly.\n",
    "\n",
    "4. SHORT-TERM vs LONG-TERM FAIRNESS\n",
    "   - Relaxing standards for disadvantaged groups now may perpetuate\n",
    "     stereotypes or lead to higher default rates, harming them long-term.\n",
    "\n",
    "There is no universally \"correct\" answer - these are value judgments\n",
    "that should involve stakeholders, regulators, and affected communities.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Challenge Exercises\n",
    "\n",
    "Apply what you've learned with these hands-on challenges!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Build a Fair Credit Score\n",
    "\n",
    "Implement a credit scoring model that achieves better fairness metrics while maintaining reasonable accuracy. Try techniques like:\n",
    "- Removing biased features\n",
    "- Reweighting training samples\n",
    "- Post-processing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Your code here\n",
    "\n",
    "def build_fair_model(X_train, y_train, X_test, y_test, protected_group, fairness_weight=0.5):\n",
    "    \"\"\"\n",
    "    Build a credit scoring model with fairness constraints.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        protected_group: Series indicating protected group membership\n",
    "        fairness_weight: How much to prioritize fairness vs accuracy (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, predictions, fairness metrics\n",
    "    \n",
    "    TODO: Implement a fair model using one of these approaches:\n",
    "    1. Pre-processing: Remove or modify biased features\n",
    "    2. In-processing: Add fairness constraints to the optimization\n",
    "    3. Post-processing: Adjust predictions to achieve fairness\n",
    "    \"\"\"\n",
    "    # Hints:\n",
    "    # - Try removing features correlated with protected attributes\n",
    "    # - Try reweighting samples to balance group representation\n",
    "    # - Try adjusting decision thresholds per group\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your fair model\n",
    "# fair_model, fair_preds, fair_metrics = build_fair_model(...)\n",
    "# Compare with original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Alternative Data Feature Engineering\n",
    "\n",
    "Create new features from the alternative data that might be more predictive or less biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Your code here\n",
    "\n",
    "def engineer_alternative_features(df):\n",
    "    \"\"\"\n",
    "    Create new features from alternative data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with alternative data features\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with new engineered features\n",
    "    \n",
    "    TODO: Create features like:\n",
    "    - Mobile bill to income ratio\n",
    "    - Financial app engagement score\n",
    "    - Digital footprint score (combination of online behaviors)\n",
    "    - Payment consistency index\n",
    "    \"\"\"\n",
    "    # Hints:\n",
    "    # - Combine related features (e.g., mobile_bill / annual_income)\n",
    "    # - Create interaction terms\n",
    "    # - Bin continuous features into categories\n",
    "    # - Create aggregate \"digital stability\" scores\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your features\n",
    "# df_engineered = engineer_alternative_features(df)\n",
    "# Train a model with new features and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Explainable Credit Decisions\n",
    "\n",
    "Build a system that provides explanations for credit decisions - crucial for regulatory compliance and customer trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Your code here\n",
    "\n",
    "def explain_credit_decision(model, scaler, features, applicant_data, feature_names):\n",
    "    \"\"\"\n",
    "    Generate a human-readable explanation for a credit decision.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained logistic regression model\n",
    "        scaler: StandardScaler used for training\n",
    "        features: Feature values for the applicant\n",
    "        feature_names: Names of features\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with decision and explanation\n",
    "    \n",
    "    TODO: Implement explanation logic:\n",
    "    1. Get prediction and probability\n",
    "    2. Identify top factors contributing to the decision\n",
    "    3. Generate actionable feedback for rejected applicants\n",
    "    \"\"\"\n",
    "    # Hints:\n",
    "    # - Use model coefficients to identify important features\n",
    "    # - Compare applicant values to approval thresholds\n",
    "    # - Suggest specific improvements (e.g., \"reduce debt by $X\")\n",
    "    # - Highlight both positive and negative factors\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your explanation system\n",
    "# Pick a rejected applicant and generate explanation\n",
    "# explanation = explain_credit_decision(model_combined, scaler_comb, ...)\n",
    "# print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned about credit scoring with machine learning:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Credit Scoring Fundamentals**:\n",
    "   - Traditional features: income, employment, credit history\n",
    "   - Alternative data: mobile usage, social signals, digital footprint\n",
    "   - Logistic regression for interpretable credit models\n",
    "\n",
    "2. **Model Development**:\n",
    "   - Feature engineering and selection\n",
    "   - Train-test splitting with stratification\n",
    "   - Standardization for logistic regression\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - ROC curves and AUC scores\n",
    "   - Confusion matrices\n",
    "   - Precision, recall, and F1 scores\n",
    "\n",
    "4. **Algorithmic Bias**:\n",
    "   - Sources of bias in credit models\n",
    "   - Proxy discrimination through correlated features\n",
    "   - The four-fifths (80%) rule\n",
    "\n",
    "5. **Fairness Metrics**:\n",
    "   - Statistical parity (equal approval rates)\n",
    "   - Equalized odds (equal TPR and FPR)\n",
    "   - Predictive parity (equal precision)\n",
    "   - The impossibility of satisfying all metrics simultaneously\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Consumer Lending**: Credit cards, personal loans\n",
    "- **Mortgage Underwriting**: Home loan decisions\n",
    "- **FinTech Lending**: Alternative data for thin-file borrowers\n",
    "- **Microfinance**: Credit scoring in emerging markets\n",
    "- **Buy-Now-Pay-Later**: Real-time credit decisions\n",
    "\n",
    "### Regulatory Considerations\n",
    "\n",
    "- **Equal Credit Opportunity Act (ECOA)**: Prohibits discrimination in lending\n",
    "- **Fair Credit Reporting Act (FCRA)**: Regulates credit information use\n",
    "- **EU AI Act**: Classifies credit scoring as high-risk AI\n",
    "- **Right to Explanation**: Many jurisdictions require explainable decisions\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "- Alternative data can either reduce or amplify existing inequalities\n",
    "- Accuracy improvements must be weighed against fairness implications\n",
    "- Transparency and explainability are crucial for trust\n",
    "- Historical data reflects historical biases\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Fairlearn: ML Fairness Library](https://fairlearn.org/)\n",
    "- [AI Fairness 360 (IBM)](https://aif360.mybluemix.net/)\n",
    "- [The Lending Club Dataset](https://www.kaggle.com/wordsforthewise/lending-club)\n",
    "- [CFPB Reports on Fair Lending](https://www.consumerfinance.gov/data-research/research-reports/)\n",
    "- [\"Fairness and Machine Learning\" (Barocas, Hardt, Narayanan)](https://fairmlbook.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
